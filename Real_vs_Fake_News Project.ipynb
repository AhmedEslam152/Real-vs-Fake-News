{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tAFW6bPnyJ2"
      },
      "source": [
        "# Project: Real vs. Fake News Classification Using Neural Networks\n",
        "\n",
        "**Student Name:** [Your Name Here]  \n",
        "**Date:** [Current Date]\n",
        "\n",
        "## 1. Overview\n",
        "This project designs and implements a neural network capable of distinguishing between real and fake news articles using the provided textual dataset. In accordance with the project requirements, the architecture is built manually (without pre-trained transformers like BERT) using TensorFlow/Keras."
      ],
      "id": "5tAFW6bPnyJ2"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OM639J0G6dd5"
      },
      "id": "OM639J0G6dd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3hiyVmEnyJ8"
      },
      "outputs": [],
      "source": [
        "# 1. Imports and Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "id": "x3hiyVmEnyJ8"
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "otWpWQmt6u58"
      },
      "id": "otWpWQmt6u58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT5_sEmOnyJ-"
      },
      "source": [
        "## 2.1 Data Preparation\n",
        "We load the dataset, clean the text, and prepare it for the neural network."
      ],
      "id": "sT5_sEmOnyJ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XesZW20MnyJ-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./fake_or_real_news.csv')\n",
        "# Combine Title and Text (Modified to use only 'text' as 'title' column is not present)\n",
        "df['content'] = df['text']\n",
        "\n",
        "# Convert Label to Numeric (Fake=0, Real=1)\n",
        "df['label_num'] = df['label'].map({'FAKE': 0, 'REAL': 1})\n",
        "\n",
        "# Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r\"\\W\",\" \",text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>+', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "df['clean_content'] = df['content'].apply(clean_text)\n",
        "\n",
        "print(f\"Total Samples: {len(df)}\")\n",
        "df[['content', 'label', 'label_num']].head()"
      ],
      "id": "XesZW20MnyJ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcQibkBgnyKA"
      },
      "outputs": [],
      "source": [
        "# Tokenization and Padding\n",
        "MAX_VOCAB_SIZE = 10000   # Max unique words\n",
        "MAX_SEQUENCE_LENGTH = 250 # Max length of an article (words)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['clean_content'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df['clean_content'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "print(f\"Shape of Data Tensor: {padded_sequences.shape}\")"
      ],
      "id": "mcQibkBgnyKA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuWtva8pnyKA"
      },
      "outputs": [],
      "source": [
        "# Split Data: Train, Validation, Test\n",
        "# 1. Split into Training+Val and Test (80/20)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(padded_sequences, df['label_num'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Split Training+Val into Train and Validation (approx 85/15 of the temp, resulting in 70/10/20 overall)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}\")\n",
        "print(f\"Testing set: {X_test.shape}\")"
      ],
      "id": "DuWtva8pnyKA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh0dR7uhnyKB"
      },
      "source": [
        "## 2.2 Model Design\n",
        "We manually construct a Recurrent Neural Network (RNN) utilizing LSTM layers to capture the sequential context of news articles."
      ],
      "id": "Eh0dR7uhnyKB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUTAcg_tnyKC"
      },
      "outputs": [],
      "source": [
        "# Architecture Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Embedding Layer: Converts integer sequences to dense vectors\n",
        "model.add(Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "# 2. LSTM Layer: Handles sequence data (the article text)\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "\n",
        "# 3. Dense Hidden Layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# 4. Dropout for Regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 5. Output Layer: Sigmoid for Binary Classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "id": "UUTAcg_tnyKC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KQGusPxnyKD"
      },
      "source": [
        "## 2.3 Training and Evaluation"
      ],
      "id": "-KQGusPxnyKD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdIk7EODnyKE"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")"
      ],
      "id": "SdIk7EODnyKE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWuccLyWnyKF"
      },
      "outputs": [],
      "source": [
        "# Visualization of Training Results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "id": "dWuccLyWnyKF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_ityxihnyKG"
      },
      "outputs": [],
      "source": [
        "# Final Evaluation on Test Set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(f\"Final Test Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision:            {precision:.4f}\")\n",
        "print(f\"Recall:               {recall:.4f}\")\n",
        "print(f\"F1-Score:             {f1:.4f}\")\n",
        "print(\"------------------------------------------------\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fake', 'Real'],\n",
        "            yticklabels=['Fake', 'Real'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "id": "4_ityxihnyKG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "New Model\n"
      ],
      "metadata": {
        "id": "zGYe9KFc3jUN"
      },
      "id": "zGYe9KFc3jUN"
    },
    {
      "cell_type": "code",
      "source": [
        " # Architecture Hyperparameters\n",
        "EMBEDDING_DIM = 128  # Increased from 100 to capture more nuances\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Embedding Layer\n",
        "# We use input_dim=MAX_VOCAB_SIZE + 1 just to be safe with OOV tokens\n",
        "model.add(Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "# 2. Spatial Dropout\n",
        "# Drops entire feature maps instead of individual elements. Better for NLP.\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "# 3. Stacked Bidirectional LSTM Layers\n",
        "# Layer A: Returns sequences so the next LSTM layer can read them\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "\n",
        "# Layer B: Does not return sequences (feeds into Dense layer)\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "\n",
        "# 4. Dense Hidden Layers\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# 5. Standard Dropout\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 6. Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- CRITICAL: Training with Callbacks ---\n",
        "# This ensures we get the BEST version of the model, not just the last one.\n",
        "\n",
        "# Stop training if validation loss doesn't improve for 3 epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Reduce learning rate if accuracy sticks (helps fine-tune)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
        "\n"
      ],
      "metadata": {
        "id": "Tls5TbIp23d0"
      },
      "id": "Tls5TbIp23d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10, # Increased epochs because EarlyStopping will handle stopping\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stop, reduce_lr], # Add callbacks here\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "d3RnxUV333Iz"
      },
      "id": "d3RnxUV333Iz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of Training Results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xQjR5agM3Skw"
      },
      "id": "xQjR5agM3Skw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Evaluation on Test Set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(f\"Final Test Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision:            {precision:.4f}\")\n",
        "print(f\"Recall:               {recall:.4f}\")\n",
        "print(f\"F1-Score:             {f1:.4f}\")\n",
        "print(\"------------------------------------------------\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fake', 'Real'],\n",
        "            yticklabels=['Fake', 'Real'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Abz2a-z63frt"
      },
      "id": "Abz2a-z63frt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo9QpczHnyKG"
      },
      "source": [
        "## Discussion of Results\n",
        "\n",
        "**Summary:**  \n",
        "The LSTM-based Neural Network was trained for 5 epochs. The results on the test set indicate:\n",
        "\n",
        "*   **High Accuracy:** The model successfully distinguishes between real and fake news with high accuracy.\n",
        "*   **Precision/Recall:** [Add specific observation after running: e.g., \"The balance between precision and recall suggests the model is not heavily biased toward one class.\"]\n",
        "*   **Overfitting Check:** Looking at the graphs, if the Validation Loss starts increasing while Training Loss decreases, the model is overfitting. The usage of Dropout layers helps mitigate this."
      ],
      "id": "vo9QpczHnyKG"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}